---
title: "House Prediction"
author: "Tural Ismayilov, Polad Mahmudov, Mansur Alizada"
date: "December 10, 2017"
output: pdf_document
---

**In this project named House Prediction, we will work through the House Prices: Advanced Regression Techniques competition in Kaggle.**

In order to have successful Kaggle Competition submission, we have decided to follow the steps below:

Acquire the data
Explore and clean Data
Engineer and transform the features and the target variable
Build a model
Make and submit predictions

In order to make it simple, we have gathered all libraries we used here under same chunk.
```{r}
library(tidyr)
library(dplyr)
library(data.table)
library(caret)
library(dplyr)
library(ggplot2)
library(corrplot)
```


#STEP 1: Acquire data and create our environment
We need to acquire the data for the competition. The descriptions of the features and some other helpful information are contained in
a file named, data_description.txt.

We have downloaded the data and saved it into a folder named 'data' where we will keep every data we need for the competition.

We will first look at the train.csv data where the previous sales with features were recorded. After we have trained a model, we'll
make predictions using the test.csv data.

We will use read.csv() to read csv files. The read.csv() method creates a DataFrame from a csv file.
```{r }
train <- read.csv('train.csv')
test <- read.csv('test.csv')
description <- read.csv('data description.txt')
```

Let's check the size of our data and explore it.
```{r}
length(train)
object.size(train)
length(test)
object.size(test)
```

We see that test has only 80 columns and takes 512424 bytes space, while train has 81 and takes 505488 bytes spaces. The difference in
length is of course, the fact that the test data do not include the final sale price information. But all other 80 column names are
matched in train and test data.

Next, we'll look at a few rows using the head() method.
```{r}
head(train)
```

Here's a brief version of what we will find in the data description file:

SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.
MSSubClass - The building class
MSZoning - The general zoning classification
LotFrontage - Linear feet of street connected to property
LotArea - Lot size in square feet
Street - Type of road access
Alley - Type of alley access
LotShape - General shape of property
LandContour - Flatness of the property
Utilities - Type of utilities available
LotConfig - Lot configuration

The competition is about to predict the final price of each home in test.csv. At this point, the first thing we should do is to start
thinking about what we know about housing prices, *Ames, Iowa, and what we might expect to see in this dataset.

Looking at the data, we see the features we have expected, like YrSold (the year the home was last sold), GarageArea, SalePrice and so
on. Others we might not have anticipated, such as LandSlope (the slope of the land the home is built upon) and RoofMatl (the materials
used to construct the roof). Later, we'll have to make decisions about how we'll approach these and other features. Because there can
be some features that are negatively correlated with SalesPrice.

We want to do some plotting during the exploration stage of our project, and we'll need to import that functionality into our
environment as well. Plotting allows us to visualize the distribution of the data, check for outliers, and see other patterns that we
might miss otherwise. 

#STEP 2: Explore the data and engineer Features
The challenge is to predict the final sale price of the homes. This information is stored in the SalePrice column. The value we are
trying to predict is often called the target variable. As we expect, on of the most important column in our train dataset is
SalesPrice, so let's look at the distribution of it.

We can use Series.describe() to get more information.


```{r}
summary(train$SalePrice)
```
For numerical data, summary() gives the mean, median, min, max and some other values.

The mean value sale price of a house in our dataset is close to 180 000$ with the most values ranging between 129 975$ and 214 000 $.
```{r}
hist(train$SalePrice,main="Sale Prices of Houses",ylab="Number of times sold",xlab="Sale Prices",col="blue3")
```

We used hist() to plot a histogram of SalePrice. It can be noticed that the distribution has a longer tail on the right. The
distribution is positively skewed.

##1: Working with numeric features
```{r}
numeric_features <- sapply(train, is.numeric)
str(train[numeric_features])
```

The cor() method displays the correlation between the columns. We will examine the correlations between the features and the target.
So we will be able to find the most and the least correlated values.

```{r}
sort(cor(train[numeric_features])['SalePrice',])
```

The first five features are the most negatively correlated with SalePrice, while the next five are the most positively correlated.

We can create a pivot table to further investigate the relationship between let's say OverallQual and SalePrice to see the
correlation. We will first find the unique elements in OverallQual then plot the median for each element.
```{r}
unique(train$OverallQual)
```

We can see that it ranges from 1 to 10.
```{r}
train %>%
  group_by(OverallQual) %>%
    summarise(Mean = mean(SalePrice)) -> train_mean_saleprice

ggplot(train_mean_saleprice, aes(OverallQual, Mean)) + geom_histogram(stat = "identity") + scale_x_continuous(breaks=1:10) 
```

Here we used ggplot of ggplot2 library to display the distribution of median for grouped elements. We can see that the median sales
price strictly increases as Overall Quality increases. It proves the positive correlation.

Now we can look at other features to understand the correleation well. We will take firt 5 features that we expect has positive
relationship with Sale Price. These features are GrLivArea, LotArea, LotFrontage, GarageArea.

```{r}
p1 <- subset(train, !is.na(GrLivArea))
p1 <- ggplot(p1, aes(GrLivArea, SalePrice)) + geom_point(color = 'red') + theme_bw()
p2 <- subset(train, !is.na(LotArea))
p2 <- ggplot(p2, aes(LotArea, SalePrice)) + geom_point(color = 'red') + theme_bw()
p3 <- subset(train, !is.na(LotFrontage))
p3 <- ggplot(p3, aes(LotFrontage, SalePrice)) + geom_point(color = 'red') + theme_bw()
p4 <- subset(train, !is.na(GarageArea))
p4 <- ggplot(p4, aes(GarageArea, SalePrice)) + geom_point(color = 'red') + theme_bw()

p1
p2
p3
p4

#Now we can remove the outliers
train <- subset(train, GrLivArea < 4000 | is.na(GrLivArea))
train <- subset(train, LotArea < 100000 | is.na(LotArea))
train <- subset(train, LotFrontage < 200 | is.na(LotFrontage))
train <- subset(train, GarageArea < 1500 | is.na(GarageArea))
```

Now we can plot histogram and see the outliers in the features we are going to choose.

Now we can divide the columns into Numeric and Factor type. We will use dataType vector and colInfo dataframe to determine it.
```{r}
dataType <- c('NAN','F','F','N','N','F','F','F','F','F','F','F',
                'F','F','F','F','F','F','F','F','F','F','F','F',
                'F','F','N','F','F','F','F','F','F','F','N','F',
                'N','N','N','F','F','F','F','N','N','N','N','F',
                'F','F','F','F','F','F','F','F','F','F','F','F',
                'F','F','N','F','F','F','N','N','N','N','N','N',
                'F','F','F','N','F','F','F','F')

colInfo <- data.frame(No = seq(1:80), 
                       Column = names(test),
                       DataType = dataType)
```

##2: Handling Null Values
Let's examine null values. To start with dealing null values, let's report top null values by returning Series of the counts of the null values in each column.
```{r}
sort(colSums(is.na(train)))
```

If we look at the data description.txt file, we can notice that, in the case of PoolQC, this column refers to Pool Quality.

PoolQC: Pool quality
		
       Ex -	Excellent
       Gd	- Good
       TA	- Average/Typical
       Fa	- Fair
       NA	- No Pool
       
In that column having NaN means that no or 0 pool. So removing this NaNs can result bad prediction. Therefore changing these values to
None, in the case of PoolQC is better. We can see similar relationship between many of the columns.
```{r}
#We will combine test and train set and do data cleaning. It will help us to predict better
comb <- rbind(train[,-c(ncol(train))], test)

# Assume 0 linear feet of street connected to property if NA
comb$LotFrontage[is.na(comb$LotFrontage)] <- 0

# These factors contains NA values which mean the property has no such facility
# Becase the column is factor, in order to change NA to None we should add None level, then change all NAs to None
# We will make method for it called nullToY

nullToY <- function(x, y) {
  x <- as.character(x)
  x[is.na(x)] <- y
  x <- as.factor(x)
  return(x)
}
 
comb$MiscFeature <- nullToY(comb$MiscFeature, 'None')
comb$Fence <- nullToY(comb$Fence, 'None')
comb$PoolQC <- nullToY(comb$PoolQC, 'None')
comb$FireplaceQu <- nullToY(comb$FireplaceQu, 'None')
comb$Alley <- nullToY(comb$Alley, 'None')

# Unknown categorical feature set to a new category value - OTHER
comb$SaleType <- nullToY(comb$SaleType, 'Oth')
comb$MSZoning <- nullToY(comb$MSZoning, 'OTH')
comb$Exterior1st <- nullToY(comb$Exterior1st, 'Other')
comb$Exterior2nd <- nullToY(comb$Exterior2nd, 'Other')
comb$Functional <- nullToY(comb$Functional, 'Oth')

# Unknown categorical feature set to most common categorical value
comb$Utilities[is.na(comb$Utilities)] <- comb$Utilities[which.max(comb$Utilities)]
comb$Electrical[is.na(comb$Electrical)] <- comb$Electrical[which.max(comb$Electrical)]
comb$KitchenQual[is.na(comb$KitchenQual)] <- comb$KitchenQual[which.max(comb$KitchenQual)]

# If both MasVnrType and MasVnrArea columns are NA, Area should be 0 and Type should be None
comb[is.na(comb$MasVnrType) & is.na(comb$MasVnrArea),]$MasVnrArea <- 0
comb[is.na(comb$MasVnrType) & comb$MasVnrArea >= 0,]$MasVnrType <- 'None'
comb[comb$MasVnrType %in% 'None' & comb$MasVnrArea >= 0,]$MasVnrArea <- 0
comb[!comb$MasVnrType %in% 'None' & comb$MasVnrArea == 0,]$MasVnrType <- 'None'

# if Area is 0, the rest of the Garage feature should be None
# if Area is 0, the rest of the Garage feature should be None
comb$GarageType <- nullToY(comb$GarageType, 'None')
comb$GarageFinish <- nullToY(comb$GarageFinish, 'None')
comb$GarageQual <- nullToY(comb$GarageQual, 'None')
comb$GarageCond <- nullToY(comb$GarageCond, 'None')
comb$GarageYrBlt <-nullToY(comb$GarageYrBlt, '0') 
comb[is.na(comb$GarageCars),]$GarageCars <- 0
comb[is.na(comb$GarageArea),]$GarageArea <- 0

#if GareageArea is 0, so the GarageType is None
comb[comb$GarageArea == 0,]$GarageType <- 'None'

comb[comb$GarageYrBlt %in% '0' & 
          comb$GarageType %in% 'Detchd' & 
          comb$GarageCars > 0 & 
          comb$GarageArea > 0,]$GarageFinish <- comb$GarageFinish[which.max(comb$GarageFinish)]

comb[comb$GarageYrBlt %in% '0' & 
          comb$GarageType %in% 'Detchd' & 
          comb$GarageCars > 0 & 
          comb$GarageArea > 0,]$GarageQual <- comb$GarageQual[which.max(comb$GarageQual)]

comb[comb$GarageYrBlt %in% '0' & 
          comb$GarageType %in% 'Detchd' & 
          comb$GarageCars > 0 & 
          comb$GarageArea > 0,]$GarageCond <- comb$GarageCond[which.max(comb$GarageCond)]

# if Area is 0, the rest of the Basement feature should be None
comb$BsmtQual <- nullToY(comb$BsmtQual, 'None')
comb$BsmtCond <- nullToY(comb$BsmtCond, 'None')
comb$BsmtExposure <- nullToY(comb$BsmtExposure, 'None')
comb$BsmtFinType1 <- nullToY(comb$BsmtFinType1, 'None')
comb$BsmtFinType2 <- nullToY(comb$BsmtFinType2, 'None')

comb[is.na(comb$TotalBsmtSF),]$BsmtFinSF1 <- 0
comb[is.na(comb$TotalBsmtSF),]$BsmtFinSF2 <- 0
comb[is.na(comb$TotalBsmtSF),]$BsmtUnfSF <- 0
comb[is.na(comb$TotalBsmtSF),]$BsmtFullBath <- 0
comb[is.na(comb$TotalBsmtSF),]$BsmtHalfBath <- 0
comb[is.na(comb$TotalBsmtSF),]$TotalBsmtSF <- 0

comb[comb$TotalBsmtSF == 0,]$BsmtFullBath  <- 0
comb[comb$TotalBsmtSF == 0,]$BsmtHalfBath <- 0
```

Now let's examine the correlation between numeric features and Sale Price. It is useful to make 2 numeric features that contains the total are of property and total area of 1st and 2nd floor. It will lead to better prediction.
```{r}
trainNum <- comb[,colInfo[colInfo$DataType=="N",]$No]

#Adding the useful nueric features
trainNum$TotalArea <- trainNum$LotFrontage + trainNum$LotArea + trainNum$MasVnrArea + trainNum$BsmtFinSF1 + 
                    trainNum$BsmtFinSF2 + trainNum$BsmtUnfSF + trainNum$TotalBsmtSF + trainNum$X1stFlrSF + 
                    trainNum$X2ndFlrSF + trainNum$GrLivArea + trainNum$GarageArea + trainNum$WoodDeckSF +
                    trainNum$OpenPorchSF + trainNum$EnclosedPorch + trainNum$X3SsnPorch + 
                    trainNum$ScreenPorch + trainNum$LowQualFinSF + trainNum$PoolArea

trainNum$TotalArea1st2nd <- trainNum$X1stFlrSF + trainNum$X2ndFlrSF

trainNumLowered <- trainNum[,1:(ncol(trainNum)-2)]
trainNumLowered <- trainNumLowered[1:nrow(train),]
trainNumLowered$SalePrice <- train$SalePrice

corrplot(cor(trainNumLowered), type="lower", order="hclust")
```


#STEP 3: Engineer and transform the features and the target variable
##1: Numeric features
```{r}
#Removing some features regarding Correlation Analysis
trainNum <- trainNum[,!colnames(trainNum) %in% 'EnclosedPorch']
trainNum <- trainNum[,!colnames(trainNum) %in% 'LowQualFinSF']
trainNum <- trainNum[,!colnames(trainNum) %in% 'MiscVal']
trainNum <- trainNum[,!colnames(trainNum) %in% 'OpenPorchSF']
trainNum <- trainNum[,!colnames(trainNum) %in% 'PoolArea']
trainNum <- trainNum[,!colnames(trainNum) %in% 'ScreenPorch']
trainNum <- trainNum[,!colnames(trainNum) %in% 'X3SsnPorch']

#Now it is time to normalise the numeric features
trainNum <- data.frame(lapply(trainNum, function(x) {log1p(x)}))
```

##2: Categorical features
We grouped the months with high sales transactions with categorical features. Months with high transactions can have an impact to Sale
Price.
```{r}
#First identify the categorical features
trainFac <- comb[,colInfo[colInfo$DataType=="F",]$No]
trainFac <- data.frame(lapply(trainFac, as.factor))
ncol(trainFac)

anly_data <- train %>% group_by(MoSold) %>% summarise(Count = n())
anly_data$MoSold <- as.factor(anly_data$MoSold)
ggplot(data=anly_data, aes(x=MoSold, y=Count)) + geom_bar(stat="identity", fill="steelblue") + theme_minimal()
```

We can observe that in the middle of the year the counts are higher than others. So we will determine it as 0 or 1 showing whether 
the counts are high or not.
```{r}
convertFeature <- function (colname, input) {
    colname <- as.character(colname)
    for (s in input) {
        s <- strsplit(s,":")[[1]]
        fac <- s[1]
        num <- s[2]
        colname <- replace(colname, colname == fac , num)
    }
    colname
}

trainFac$PopularMonth <- as.factor(convertFeature(trainFac$MoSold, 
                                                c('1:0','2:0','3:0','4:1',
                                                  '5:1','6:1','7:1','8:0',
                                                  '9:0','10:0','11:0','12:0')))
```

We know that certain types of dwellings can be popular and therefore may have an impact to Sale Price. So we created new categorical
feature by grouping types of dwellings with high transactions.
```{r}
anly_data <- train %>% group_by(MSSubClass) %>% summarise(Count = n())
anly_data$MSSubClass <- as.factor(anly_data$MSSubClass)
ggplot(data=anly_data, aes(x=MSSubClass, y=Count)) + geom_bar(stat="identity", fill="steelblue")+ theme_minimal()
```

Now we can add new vector that shows whether given type of dwelling is popular or not.
```{r}
trainFac$PopularDwelling <- as.factor(convertFeature(trainFac$MSSubClass, 
                                                     c('20:1','30:0','40:0','45:0','50:1',
                                                       '60:1','70:0','75:0','80:0','85:0',
                                                       '90:0','120:1','150:0','160:0',
                                                       '180:0','190:0')))


studydata <- trainFac 
factorInfo <- data.frame(ColNames = colnames(studydata), 
                         NLevels = as.numeric(as.character(
                             apply(studydata, 2, function(x) {nlevels(as.factor(x))}))))
```

##3: More feature engineering
Converting caterogical features to numeric one is good for our prediction that it will make it more accurate. So we have decided
to do that.
```{r}
catToNum <- c('None:0','Po:1','Fa:2','TA:3','Gd:4','Ex:5')

trainNum$BsmtExposure <- as.numeric(convertFeature(trainFac$BsmtExposure, 
                            c('None:0','No:1','Mn:2','Av:3','Gd:4')))
trainNum$BsmtFinType1 <- as.numeric(convertFeature(trainFac$BsmtFinType1, 
                            c('None:0','Unf:1','LwQ:2','Rec:3','BLQ:4','ALQ:5','GLQ:6')))
trainNum$BsmtFinType2 <- as.numeric(convertFeature(trainFac$BsmtFinType2, 
                            c('None:0','Unf:1','LwQ:2','Rec:3','BLQ:4','ALQ:5','GLQ:6')))
trainNum$Functional <- as.numeric(convertFeature(trainFac$Functional, 
                            c('Oth:0','Sal:1','Sev:2','Maj2:3','Maj1:4','Mod:5','Min2:6','Min1:7','Typ:8')))
trainNum$GarageFinish <- as.numeric(convertFeature(trainFac$GarageFinish, 
                            c('None:0','Unf:1','RFn:2','Fin:3')))
trainNum$Fence <- as.numeric(convertFeature(trainFac$Fence, 
                            c('None:0','MnWw:1','GdWo:2','MnPrv:3','GdPrv:4')))
trainNum$ExterQual <- as.numeric(convertFeature(trainFac$ExterQual, catToNum))
trainNum$ExterCond <- as.numeric(convertFeature(trainFac$ExterCond, catToNum))
trainNum$BsmtQual <- as.numeric(convertFeature(trainFac$BsmtQual, catToNum))
trainNum$BsmtCond <- as.numeric(convertFeature(trainFac$BsmtCond, catToNum))
trainNum$HeatingQC <- as.numeric(convertFeature(trainFac$HeatingQC, catToNum))
trainNum$KitchenQual <- as.numeric(convertFeature(trainFac$KitchenQual, catToNum))
trainNum$FireplaceQu <- as.numeric(convertFeature(trainFac$FireplaceQu, catToNum))
trainNum$GarageQual <- as.numeric(convertFeature(trainFac$GarageQual, catToNum))
trainNum$GarageCond <- as.numeric(convertFeature(trainFac$GarageCond, catToNum))
trainNum$PoolQC <- as.numeric(convertFeature(trainFac$PoolQC, catToNum))

#Again we categorized them
trainNum$OverallQual2 <- as.numeric(convertFeature(trainFac$OverallQual,
                                    c('1:1','2:1','3:1','4:2','5:2','6:2','7:3','8:3','9:3','10:3')))
trainNum$OverallCond2 <- as.numeric(convertFeature(trainFac$OverallCond,
                                    c('1:1','2:1','3:1','4:2','5:2','6:2','7:3','8:3','9:3','10:3')))
trainNum$OverallQual <- as.numeric(as.character(trainFac$OverallQual))
trainNum$OverallCond <- as.numeric(as.character(trainFac$OverallCond))

#This function is for 
binaryFeature <- function (col, needFac) {
    output <- as.character(col)
    output <- sapply(output, function(x) as.numeric(ifelse(x %in% needFac,1,0)))
    as.factor(output)
}

# If the price is a discounted price so 1 otherwise 0.
trainFac$SaleNormal <- binaryFeature(trainFac$SaleCondition, c('Normal','Partial'))

# If it is uncompleted saleIs then 1 otherwise 0.
trainFac$SaleComplete <- binaryFeature(trainFac$SaleCondition, 'Partial')

# If the shape is regular then 1 otherwise 0.
trainFac$IsRegularShape <- binaryFeature(trainFac$LotShape, 'Reg')

# If the zone is prime then 1 otherwise 0.
trainFac$IsPrimeZone <- binaryFeature(trainFac$MSZoning, c('FV','RL','RP'))

# If it is Normal Zone then 1 otherwise 0.
trainFac$IsNormalZone <- binaryFeature(trainFac$MSZoning, c('RH','RM'))

## Based on mean Sale Price we grouped Neighborhood 
trainNum$NeighborhoodBin <- as.numeric(convertFeature(trainFac$Neighborhood,
                                c('MeadowV:0','IDOTRR:1','BrDale:1','OldTown:1','Edwards:1',
                                  'BrkSide:1','Sawyer:1','Blueste:1','SWISU:2','NAmes:2',
                                  'NPkVill:2','Mitchel:2','SawyerW:2','Gilbert:2','NWAmes:2',
                                  'Blmngtn:2','CollgCr:2','ClearCr:3','Crawfor:3',
                                  'Veenker:3','Somerst:3','Timber:3','StoneBr:4','NoRidge:4','NridgHt:4')))

# How old is the property when it was sold?
trainNum$Age <- as.numeric(as.character(trainFac$YrSold)) - as.numeric(as.character(trainFac$YearBuilt))

# Convert these categorial feature to numeric because there is an expectation that higher the count, higher the Sale Price 
trainNum$BsmtBath <- as.numeric(as.character(trainFac$BsmtFullBath)) + as.numeric(as.character(trainFac$BsmtHalfBath)) * 0.5
trainNum$Bath <- as.numeric(as.character(trainFac$FullBath)) + as.numeric(as.character(trainFac$HalfBath)) * 0.5
trainNum$TotalBath <- trainNum$BsmtBath + trainNum$Bath
trainNum$BedroomAbvGr <- as.numeric(as.character(trainFac$BedroomAbvGr))
trainNum$KitchenAbvGr <- as.numeric(as.character(trainFac$KitchenAbvGr))
trainNum$TotRmsAbvGrd <- as.numeric(as.character(trainFac$TotRmsAbvGrd))
trainNum$Fireplaces <- as.numeric(as.character(trainFac$Fireplaces))
trainNum$GarageCars <- as.numeric(as.character(trainFac$GarageCars))

normalize <- function(x) {
    return ((x - min(x)) / (max(x) - min(x)))
}

# Creating a timestamp feature for each transaction.
trainNum$DtSold <- as.numeric(as.character(trainFac$YrSold)) + as.numeric(as.character(trainFac$MoSold))/12
trainNum$DtSold <- normalize(trainNum$DtSold)
```

##4: Feature Elimination
With new features created, deleting some features that may no longer be useful.
```{r}
trainFac <- trainFac[,!colnames(trainFac) %in% 'LotShape']
trainFac <- trainFac[,!colnames(trainFac) %in% 'LandContour']
trainFac <- trainFac[,!colnames(trainFac) %in% 'LandSlope']
trainFac <- trainFac[,!colnames(trainFac) %in% 'Electrical']
trainFac <- trainFac[,!colnames(trainFac) %in% 'GarageType']
trainFac <- trainFac[,!colnames(trainFac) %in% 'PavedDrive']
trainFac <- trainFac[,!colnames(trainFac) %in% 'MiscFeature']
trainFac <- trainFac[,!colnames(trainFac) %in% 'Neighborhood']
trainFac <- trainFac[,!colnames(trainFac) %in% 'MSSubClass']
trainFac <- trainFac[,!colnames(trainFac) %in% 'YrSold']
trainFac <- trainFac[,!colnames(trainFac) %in% 'MoSold']
trainFac <- trainFac[,!colnames(trainFac) %in% 'YearRemodAdd']
trainFac <- trainFac[,!colnames(trainFac) %in% 'YearBuilt']
trainFac <- trainFac[,!colnames(trainFac) %in% 'BsmtFullBath']
trainFac <- trainFac[,!colnames(trainFac) %in% 'BsmtHalfBath']
trainFac <- trainFac[,!colnames(trainFac) %in% 'FullBath']
trainFac <- trainFac[,!colnames(trainFac) %in% 'HalfBath']
trainFac <- trainFac[,!colnames(trainFac) %in% 'BedroomAbvGr']
trainFac <- trainFac[,!colnames(trainFac) %in% 'KitchenAbvGr']
trainFac <- trainFac[,!colnames(trainFac) %in% 'TotRmsAbvGrd']
trainFac <- trainFac[,!colnames(trainFac) %in% 'Fireplaces']
trainFac <- trainFac[,!colnames(trainFac) %in% 'OverallQual']
trainFac <- trainFac[,!colnames(trainFac) %in% 'OverallCond']
trainFac <- trainFac[,!colnames(trainFac) %in% 'BsmtQual']
trainFac <- trainFac[,!colnames(trainFac) %in% 'BsmtCond']
trainFac <- trainFac[,!colnames(trainFac) %in% 'HeatingQC']
trainFac <- trainFac[,!colnames(trainFac) %in% 'KitchenQual']
trainFac <- trainFac[,!colnames(trainFac) %in% 'FireplaceQu']
trainFac <- trainFac[,!colnames(trainFac) %in% 'GarageQual']
trainFac <- trainFac[,!colnames(trainFac) %in% 'GarageCond']
trainFac <- trainFac[,!colnames(trainFac) %in% 'PoolQC']
trainFac <- trainFac[,!colnames(trainFac) %in% 'BsmtExposure']
trainFac <- trainFac[,!colnames(trainFac) %in% 'BsmtFinType1']
trainFac <- trainFac[,!colnames(trainFac) %in% 'BsmtFinType2']
trainFac <- trainFac[,!colnames(trainFac) %in% 'Functional']
trainFac <- trainFac[,!colnames(trainFac) %in% 'GarageFinish']
trainFac <- trainFac[,!colnames(trainFac) %in% 'Fence']
trainFac <- trainFac[,!colnames(trainFac) %in% 'GarageYrBlt']
trainFac <- trainFac[,!colnames(trainFac) %in% 'GarageCars']
trainNum <- trainNum[,!colnames(trainNum) %in% 'X2ndFlrSF']
trainNum <- trainNum[,!colnames(trainNum) %in% 'MasVnrArea']
trainNum <- trainNum[,!colnames(trainNum) %in% 'WoodDeckSF']

trainFac$ExterQual <- NULL
trainFac$ExterCond <- NULL
```

#STEP 4: Building a model

Now we can recombine trainNum and trainFac to make train data, then we should add log of Sale Price to newly made train data. 

```{r, warning=FALSE, message=FALSE}
combi2nd <- cbind(trainNum,trainFac)

train <- cbind(combi2nd[1:nrow(train),],SalePrice = train[,c(ncol(train))])
test <- combi2nd[(nrow(train)+1):nrow(combi2nd),]

train$SalePrice <- log1p(train$SalePrice)

tr.control <- trainControl(method="repeatedcv", number = 100,repeats = 100)
lasso_model <- train(SalePrice~., data=train,method="glmnet",metric="RMSE",
                      maximize=FALSE,trControl=tr.control,
                      tuneGrid=expand.grid(alpha=1,lambda=c(1,0.1,0.05,0.01,seq(0.009,0.001,-0.001), 0.00075,0.0005,0.0001)))

lassopreds <- exp(predict(lasso_model, test))-1
```

#STEP 5: Make and submit predictions
```{r}
testing <- read.csv('data/test.csv')
write.csv(data.frame(Id=testing$Id,SalePrice=lassopreds),"lasso_5.csv",row.names = F)
```
