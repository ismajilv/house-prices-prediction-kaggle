---
title: "House Prediction"
author: "Tural Ismayilov, Polad Mahmudov, Mansur Alizada"
date: "December 10, 2017"
output: pdf_document
---

**In this project named House Prediction, we will work through the House Prices: Advanced Regression Techniques competition in Kaggle.**

In order to have successful Kaggle Competition submission, we have decided to follow the steps below:

Acquire the data
Explore and clean Data
Engineer and transform the features and the target variable
Build a model
Make and submit predictions

In order to make it simple, we have gathered all libraries we used here under same chunk.
```{r}
library(tidyr)
library(dplyr)
library(data.table)
library(caret)
library(dplyr)
library(ggplot2)
library(corrplot)
```


#STEP 1: Acquire data and create our environment
We need to acquire the data for the competition. The descriptions of the features and some other helpful information are contained in
a file named, data_description.txt.

We have downloaded the data and saved it into a folder named 'data' where we will keep every data we need for the competition.

We will first look at the train.csv data where the previous sales with features were recorded. After we have trained a model, we'll
make predictions using the test.csv data.

We will use read.csv() to read csv files. The read.csv() method creates a DataFrame from a csv file.
```{r }
train <- read.csv('data/train.csv')
test <- read.csv('data/test.csv')
description <- read.csv('data/data description.txt')
```

Let's check the size of our data and explore it.
```{r}
length(train)
object.size(train)
length(test)
object.size(test)
```

We see that test has only 80 columns and takes 512424 bytes space, while train has 81 and takes 505488 bytes spaces. The difference in
length is of course, the fact that the test data do not include the final sale price information. But all other 80 column names are
matched in train and test data.

Next, we'll look at a few rows using the head() method.
```{r}
head(train)
```

Here's a brief version of what we will find in the data description file:

SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.
MSSubClass - The building class
MSZoning - The general zoning classification
LotFrontage - Linear feet of street connected to property
LotArea - Lot size in square feet
Street - Type of road access
Alley - Type of alley access
LotShape - General shape of property
LandContour - Flatness of the property
Utilities - Type of utilities available
LotConfig - Lot configuration

The competition is about to predict the final price of each home in test.csv. At this point, the first thing we should do is to start
thinking about what we know about housing prices, *Ames, Iowa, and what we might expect to see in this dataset.

Looking at the data, we see the features we have expected, like YrSold (the year the home was last sold), GarageArea, SalePrice and so
on. Others we might not have anticipated, such as LandSlope (the slope of the land the home is built upon) and RoofMatl (the materials
used to construct the roof). Later, we'll have to make decisions about how we'll approach these and other features. Because there can
be some features that are negatively correlated with SalesPrice.

We want to do some plotting during the exploration stage of our project, and we'll need to import that functionality into our
environment as well. Plotting allows us to visualize the distribution of the data, check for outliers, and see other patterns that we
might miss otherwise. 

#STEP 2: Explore the data and engineer Features
The challenge is to predict the final sale price of the homes. This information is stored in the SalePrice column. The value we are
trying to predict is often called the target variable. As we expect, on of the most important column in our train dataset is
SalesPrice, so let's look at the distribution of it.

We can use Series.describe() to get more information.


```{r}
summary(train$SalePrice)
```
For numerical data, summary() gives the mean, median, min, max and some other values.

The mean value sale price of a house in our dataset is close to 180 000$ with the most values ranging between 129 975$ and 214 000 $.
```{r}
hist(train$SalePrice,main="Sale Prices of Houses",ylab="Number of times sold",xlab="Sale Prices",col="blue3")
```

We used hist() to plot a histogram of SalePrice. It can be noticed that the distribution has a longer tail on the right. The
distribution is positively skewed.

##1: Working with numeric features
```{r}
numeric_features <- sapply(train, is.numeric)
str(train[numeric_features])
```

The cor() method displays the correlation between the columns. We will examine the correlations between the features and the target.
So we will be able to find the most and the least correlated values.

```{r}
sort(cor(train[numeric_features])['SalePrice',])
```

The first five features are the most negatively correlated with SalePrice, while the next five are the most positively correlated.

We can create a pivot table to further investigate the relationship between let's say OverallQual and SalePrice to see the
correlation. We will first find the unique elements in OverallQual then plot the median for each element.
```{r}
unique(train$OverallQual)
```

We can see that it ranges from 1 to 10.
```{r}
train %>%
  group_by(OverallQual) %>%
    summarise(Mean = mean(SalePrice)) -> train_mean_saleprice

ggplot(train_mean_saleprice, aes(OverallQual, Mean)) + geom_histogram(stat = "identity") + scale_x_continuous(breaks=1:10) 
```

Here we used ggplot of ggplot2 library to display the distribution of median for grouped elements. We can see that the median sales
price strictly increases as Overall Quality increases. It proves the positive correlation.

Now we can look at other features to understand the correleation well. We will take firt 5 features that we expect has positive
relationship with Sale Price. These features are GrLivArea, LotArea, LotFrontage, GarageArea.

```{r}
p1 <- subset(train, !is.na(GrLivArea))
p1 <- ggplot(p1, aes(GrLivArea, SalePrice)) + geom_point(color = 'red') + theme_bw()
p2 <- subset(train, !is.na(LotArea))
p2 <- ggplot(p2, aes(LotArea, SalePrice)) + geom_point(color = 'red') + theme_bw()
p3 <- subset(train, !is.na(LotFrontage))
p3 <- ggplot(p3, aes(LotFrontage, SalePrice)) + geom_point(color = 'red') + theme_bw()
p4 <- subset(train, !is.na(GarageArea))
p4 <- ggplot(p4, aes(GarageArea, SalePrice)) + geom_point(color = 'red') + theme_bw()

p1
p2
p3
p4

#Now we can remove the outliers
train <- subset(train, GrLivArea < 4000 | is.na(GrLivArea))
train <- subset(train, LotArea < 100000 | is.na(LotArea))
train <- subset(train, LotFrontage < 200 | is.na(LotFrontage))
train <- subset(train, GarageArea < 1500 | is.na(GarageArea))
```

Now we can plot histogram and see the outliers in the features we are going to choose.

Now we can divide the columns into Numeric and Factor type. We will use dataType vector and colInfo dataframe to determine it.
```{r}
dataType <- c('NAN','F','F','N','N','F','F','F','F','F','F','F',
                'F','F','F','F','F','F','F','F','F','F','F','F',
                'F','F','N','F','F','F','F','F','F','F','N','F',
                'N','N','N','F','F','F','F','N','N','N','N','F',
                'F','F','F','F','F','F','F','F','F','F','F','F',
                'F','F','N','F','F','F','N','N','N','N','N','N',
                'F','F','F','N','F','F','F','F')

colInfo <- data.frame(No = seq(1:80), 
                       Column = names(test),
                       DataType = dataType)
```
